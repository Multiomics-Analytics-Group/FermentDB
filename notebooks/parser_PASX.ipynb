{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea153251",
   "metadata": {},
   "source": [
    "# Parser for PAS-X Data File\n",
    "This notebook focuses on parsing data from PAS-X software to extract and model/organize high-cell density fermentation data.  \n",
    "\n",
    "PAS-X file contains data about one project which contains several runs, and each run contains data of several features.   \n",
    "\n",
    "The <b>primary objective</b> is to parse the PAS-X data file with configs files in order to generate document structures (JSON) for our graph, concretely node documents and edge documents such as: project, run, strain, species, etc.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b492d2",
   "metadata": {},
   "source": [
    "### 0. Install dependencies, import modules and define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b96a6a9-cc90-46f7-9a29-ed9e73623d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install pyarango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ae25ad-ad5c-4bff-957c-1d9a124d7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import hashlib\n",
    "from functools import reduce\n",
    "import operator\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c3e111-55d5-4282-a0eb-515d579d85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths\n",
    "data_dir = '../data'\n",
    "data_path = os.path.join(data_dir, \"20240328_dataset for ambrDB_DDBproject.json\")\n",
    "strain_mapping_path = os.path.join(data_dir, \"strain_metadata_benchling.csv\")\n",
    "taxa_mapping_path = os.path.join(data_dir, \"organism_metadata_benchling.csv\")\n",
    "project_metadata_mapping_path = os.path.join(data_dir, \"FermentDB_metadata_project_metadata.csv\")\n",
    "medium_mapping_path = os.path.join(data_dir, \"medium_metadata_benchling.csv\")\n",
    "ingredient_mapping_path = os.path.join(data_dir, \"FermentDB_metadata_medium_concentrations.csv\")\n",
    "experiment_mapping_path = os.path.join(data_dir, \"experiment_metadata_benchling.csv\")\n",
    "var_description_mapping_path = os.path.join(\"../notebooks/bioprocess_ontology/bioprocess_variables_description.json\")\n",
    "imodulon_files = {'e_coli': {'precise1k': os.path.join(data_dir, 'iM_table.csv')}}\n",
    "\n",
    "config_dir = '../config'\n",
    "main_config_path = os.path.join(config_dir, 'main_parser_config.yaml')\n",
    "#var_config_path = os.path.join(config_dir, 'var_parser_config.yaml')\n",
    "\n",
    "\n",
    "# Output paths\n",
    "output_dir = '../output'\n",
    "nodes_path = os.path.join(output_dir, \"fermentdb_nodes_full.json\")\n",
    "edges_path = os.path.join(output_dir, \"fermentdb_edges_full.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5b857",
   "metadata": {},
   "source": [
    "### 1. Load data and config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34617eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['end', 'created_by', 'editable', 'unit_operations', 'workflow', 'creation_time', 'project', 'description', 'status', 'name', 'event_names', 'key_variable', 'id', 'type', 'result_notes', 'progress', 'country', 'manager', 'trending_settings', 'deletable', 'modification_time', 'batch_phase_names', 'tags', 'start', 'departments', 'editable_plots', 'sites', 'batches'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PASX data file\n",
    "with open(data_path, 'r') as dbfile:\n",
    "    data = json.load(dbfile)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42640478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Project': {'_key': 'project',\n",
       "  'id': 'project',\n",
       "  'creation_time': 'creation_time'},\n",
       " 'Country': {'_key': 'country', 'id': 'country', 'name': 'country'},\n",
       " 'User': {'_key': 'manager',\n",
       "  'id': 'manager',\n",
       "  'name': 'manager',\n",
       "  'surname': 'manager'},\n",
       " 'Experiment': {'batches': {'variables': {'_key': 'Experiment',\n",
       "    'name': 'Experiment'}}},\n",
       " 'Run': {'batches': {'_key': 'id',\n",
       "   'id': 'id',\n",
       "   'name': 'name',\n",
       "   'run_start': 'batch_start',\n",
       "   'run_end': 'batch_end',\n",
       "   'run_date': 'creation_time',\n",
       "   'variables': {'is_control': 'Control?',\n",
       "    'replicate_number': 'Replicate #'}}},\n",
       " 'Fermenter': {'batches': {'variables': {'_key': 'Container Type',\n",
       "    'id': 'Container Type',\n",
       "    'name': 'Container Type'}}},\n",
       " 'Medium': {'batches': {'variables': {'_key': 'Base Medium',\n",
       "    'name': 'Base Medium'}}},\n",
       " 'Phase': {'batches': {'phases': {'_key': 'name',\n",
       "    'id': 'name',\n",
       "    'name': 'name'}}},\n",
       " 'Event': {'batches': {'events': {'_key': 'name',\n",
       "    'id': 'name',\n",
       "    'name': 'name'}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load main_parser_config.yaml\n",
    "with open(main_config_path, 'r') as cfile:\n",
    "    main_config = yaml.load(cfile, Loader=yaml.SafeLoader)\n",
    "\n",
    "main_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29be854",
   "metadata": {},
   "source": [
    "### 2. Create node collections dictionary structure\n",
    "Create a dictionary structure for nodes collections with main_parser_config file and PASX data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03a4becc-79a7-4191-a505-decba440eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function and Variable Definitions\n",
    "\n",
    "compound = [\"acetic acid\", \"citric acid\", \"d-glucose\", \"ethanol\", \"lactate\", \"pyruvic acid\", \"succnic acid\", \"tryptophan\", \"melatonin\"]\n",
    "substrate = [\"d-glucose\"]\n",
    "product = [\"tryptophan\", \"melatonin\", \"biomass\"]\n",
    "\n",
    "\n",
    "def get_from_nested_dict(data_dict:dict, map_list:list):\n",
    "    '''\n",
    "    Extracts value from a nested dictionary given a list of keys (different levels).\n",
    "\n",
    "    parameters:\n",
    "        data_dict (dict): dictionary structure \n",
    "        map_list (list): list of nested keys\n",
    "\n",
    "    return:\n",
    "        nested_value: nested value given the provided list of keys\n",
    "\n",
    "    example:\n",
    "        >>> data_dict = {'a': {'b': {'c': 5}}}\n",
    "        >>> value = get_from_nested_dict(data_dict=data_dict, map_list=['a', 'b', 'c'])\n",
    "        >>> print(value)\n",
    "        5\n",
    "    '''\n",
    "\n",
    "    nested_value = None\n",
    "    try:\n",
    "        nested_value = reduce(operator.getitem, map_list, data_dict)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return nested_value\n",
    "\n",
    "def get_hash(key, prefix=\"\"):\n",
    "    '''\n",
    "    Get a hash value for a given key:\n",
    "    Hash a string using SHA-1, before encode key string into bytes using the UTF-8 encoding, as the sha1() expects bytes as input. \n",
    "    Convert the binary hash value into a hexadecimal string and then into an 8-digit integer.\n",
    "    Convert it to a string and add a prefix.\n",
    "\n",
    "    parameters:\n",
    "        key (str): Input string to be hashed.\n",
    "        prefix (str, optional): Prefix to prepend to the hash value. Defaults to \"\".\n",
    "\n",
    "    return:\n",
    "        str: Hash value.\n",
    "\n",
    "    example:\n",
    "        >>> input_key = \"example_key\"\n",
    "        >>> get_hash(input_key, prefix=\"HASH_\")\n",
    "        'HASH_45200d86'\n",
    "    '''\n",
    "    hkey = str(int(hashlib.sha1(key.encode(\"utf-8\")).hexdigest(),16) % (10 ** 8))\n",
    "    hkey = f\"{prefix}{hkey}\"\n",
    "    \n",
    "    return hkey\n",
    "\n",
    "def get_unique_json_from_list_of_dicts(dict_list, unique_key='id'):\n",
    "    '''\n",
    "    Get a list of unique dictionaries based on a specified key.\n",
    "    \n",
    "    parameters:\n",
    "        dict_list (list): list of dictionaries.\n",
    "        unique_key (str, optional): Key to determine uniqueness. Default to 'id'.\n",
    "    \n",
    "    return: \n",
    "        list: list of unique dictionaries. \n",
    "    \n",
    "    example:\n",
    "        >>> example_list = [\n",
    "        ...     {\"id\": 1, \"name\": \"Albert\"},\n",
    "        ...     {\"id\": 2, \"name\": \"Chris\"},\n",
    "        ...     {\"id\": 1, \"name\": \"Charlie\"},  # Duplicate id\n",
    "        ...     {\"id\": 3, \"name\": \"Albert\"}     # Duplicate name\n",
    "        ... ]\n",
    "        >>> get_unique_json_from_list_of_dicts(example_list)\n",
    "        #Intermediate step:\n",
    "        # {1: {\"id\": 1, \"name\": \"Albert\"}, 2: {\"id\": 2, \"name\": \"Chris\"}, 3: {\"id\": 3, \"name\": Albert}}.\n",
    "        [{'id': 1, 'name': 'Albert'}, {'id': 2, 'name': 'Chris'}, {'id': 3, 'name': 'Albert'}]\n",
    "    '''\n",
    "    unique_list = list({v[unique_key]:v for v in dict_list}.values())\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "892ed61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to add update nodes to nodes_collection main function. \n",
    "\n",
    "def get_institution_node(nodes_collection:dict) -> dict:\n",
    "    #if 'Institution' not in node_collections:\n",
    "    institution_key = get_hash(\"NNFCB\", prefix=\"IN\")\n",
    "    nodes_collection['Institution'] = [{'_key': institution_key,\n",
    "                                'name': 'NNFCB - Novo Nordisk Foundation Center for Biosustainability (DTU Biosustain)',\n",
    "                                'address': 'Building 220, Kemitorvet. 2800 Kgs. Lyngby',\n",
    "                                'email': 'biosustain@biosustain.dtu.dk',\n",
    "                                'phone_number':'+45 45 25 80 00'\n",
    "                                }]\n",
    "    return nodes_collection\n",
    "\n",
    "def get_taxa_nodes(nodes_collection:dict,taxa_mapping_path:str):\n",
    "    with open(taxa_mapping_path, 'r') as taxa_file:\n",
    "        taxa_mapping = pd.read_csv(taxa_file, sep=',')\n",
    "    \n",
    "    # from df to dictionary structure\n",
    "    taxa_mapping = {r[0]: {'name': r[1], 'synonym': r[2], 'taxid': r[3]} for i,r in taxa_mapping.iterrows()}\n",
    "\n",
    "    taxas = []\n",
    "\n",
    "    for strain in nodes_collection['Strain']:\n",
    "        parent_strain = strain('parent')\n",
    "        if parent_strain in taxa_mapping:\n",
    "            taxa_id = str(taxa_mapping[parent_strain]['taxid'])\n",
    "            taxa_key = get_hash(taxa_id, prefix=\"TA\")\n",
    "            taxas.append({\n",
    "                '_key': taxa_key,\n",
    "                'id': taxa_id,\n",
    "                'source_id': parent_strain,\n",
    "                'name': str(taxa_mapping[parent_strain]['name']),\n",
    "                'synonyms': list(taxa_mapping[parent_strain]['synonym']),\n",
    "                'link': f'https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id={taxa_id}'\n",
    "\n",
    "            })\n",
    "\n",
    "    nodes_collection['Taxa'] = get_unique_json_from_list_of_dicts(dict_list=taxas,\n",
    "                                                            unique_key='_key')\n",
    "    return nodes_collection\n",
    "\n",
    "def get_strain_nodes(nodes_collection:dict, strain_mapping_path:str):\n",
    "    with open(strain_mapping_path, 'r') as strain_file:\n",
    "        strain_mapping = pd.read_csv(strain_file, sep=',')\n",
    "    \n",
    "    # from df to dictionary structure\n",
    "    strain_mapping = {r[1]: {'id': r[0], 'name': r[2], 'parent': r[3], 'host': r[4], 'genotype': r[5], 'genotype_change': r[6], 'notes': r[11]} for i,r in strain_mapping.iterrows()}\n",
    "\n",
    "    strains = []\n",
    "    for strain_key, strain_data in strain_mapping.items():\n",
    "        strain_id = str(strain_data['id'])\n",
    "        strains.append({\n",
    "            '_key': get_hash(strain_id, prefix='ST'),\n",
    "            'id': strain_id,\n",
    "            'name':  str(strain_data['name']),\n",
    "            'parent': str(strain_data['parent']),\n",
    "            'host': str(strain_data['host']),\n",
    "            'genotype': str(strain_data['genotype']),\n",
    "            'genotype_change': str(strain_data['genotype_change']),\n",
    "            'notes': str(strain_data['notes'])\n",
    "        })\n",
    "\n",
    "    nodes_collection['Strain'] = get_unique_json_from_list_of_dicts(dict_list=strains, unique_key='_key')\n",
    "\n",
    "    return nodes_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ad589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that generates all nodes\n",
    "\n",
    "def generate_nodes_collection(data:dict, config:dict, taxa_mapping_path:str) -> dict:\n",
    "    '''\n",
    "    This function creates a dictionary with the node collections expected\n",
    "    in FermentDB. It requires a dictionary with the data exported from PASX\n",
    "    in json format, and a configuration file that specifies the mapping between\n",
    "    PASX and FermentDB structure. See example in '/config/main_parser_config.yaml'\n",
    "\n",
    "    parameters:\n",
    "        data (dict): dictionary with the data read from PASX in json format\n",
    "        config (dict): mapping configuration to adapt to FermentDB structure\n",
    "\n",
    "    return:\n",
    "        collections (dict): dictionary with the expected node objects in FermentDB\n",
    "\n",
    "    example:\n",
    "        >>> get_collections_from_pasx(data=pasx_json_dict, config=main_parser_config.yaml)\n",
    "    '''\n",
    "    nodes_collection = {}\n",
    "    for key1 in config:\n",
    "        nodes_collection[key1]= []\n",
    "        collection = {}\n",
    "        for key2 in config[key1]:\n",
    "            if type(config[key1][key2]) != dict:\n",
    "                key = config[key1][key2].split('/') # returns a list even with no need to split\n",
    "                value = get_from_nested_dict(data, key)\n",
    "                if key2 == '_key':\n",
    "                    value = str(value) # keys must be str in ArangoDB\n",
    "                    if key1 == 'Project':\n",
    "                        value += str(random.random())\n",
    "                        value = get_hash(value, prefix=\"P\")\n",
    "                    elif key1 == 'Country':\n",
    "                        value = get_hash(value, prefix=\"C\")\n",
    "                    elif key1 == 'User':\n",
    "                        value = get_hash(value, prefix=\"U\")\n",
    "                collection.update({key2: value})\n",
    "            else:\n",
    "                batches = data[key2]\n",
    "                for batch in batches:\n",
    "                    collection = {}\n",
    "                    for key3 in config[key1][key2]:\n",
    "                        if type(config[key1][key2][key3]) != dict:\n",
    "                            key = config[key1][key2][key3].split('/')\n",
    "                            value = get_from_nested_dict(batch, key)\n",
    "                            if key3 == '_key':\n",
    "                                value = str(value)\n",
    "                                if key1 == 'Run':\n",
    "                                    value = get_hash(value, prefix=\"R\")\n",
    "                            collection.update({key3: value})\n",
    "                        else:\n",
    "                            variables =  batch.get(key3)\n",
    "                            for variable in variables:\n",
    "                                for key4 in config[key1][key2][key3]:\n",
    "                                    key = config[key1][key2][key3][key4]\n",
    "                                    if key == variable['name']:\n",
    "                                        value = str(variable['data'])\n",
    "                                        if key4 == '_key':\n",
    "                                            if key1 == 'Fermenter':\n",
    "                                                value = get_hash(value, prefix=\"F\")\n",
    "                                            if key1 == 'Experiment':\n",
    "                                                value = get_hash(value, prefix=\"EX\")\n",
    "                                            if key1 == 'Medium':\n",
    "                                                value = get_hash(value, prefix=\"ME\")\n",
    "                                        collection.update({key4: value})\n",
    "                                    elif key1 == \"Phase\":\n",
    "                                        key = config[key1][key2][key3][key4].split('/')\n",
    "                                        value = str(get_from_nested_dict(variable, key))\n",
    "                                        if key4 == '_key':\n",
    "                                            value += str(random.random())\n",
    "                                            value = get_hash(value, prefix=\"PH\")\n",
    "                                        collection.update({key4: value})\n",
    "                                    elif key1 == \"Event\":\n",
    "                                        key = config[key1][key2][key3][key4].split('/')\n",
    "                                        value = str(get_from_nested_dict(variable, key))\n",
    "                                        if key4 == '_key':\n",
    "                                            value += str(random.random())\n",
    "                                            value = get_hash(value, prefix=\"EV\")\n",
    "                                        collection.update({key4: value})        \n",
    "                    nodes_collection[key1].append(collection)  \n",
    "        nodes_collection[key1].append(collection)\n",
    "        #node_collections[key1] = get_unique_json_from_list_of_dicts(dict_list=node_collections[key1], unique_key='_key')\n",
    "        nodes_collection.update(get_institution_node(nodes_collection))\n",
    "        nodes_collection.update(get_taxa_nodes(nodes_collection, taxa_mapping_path))\n",
    "        nodes_collection.update(get_strain_nodes(nodes_collection, strain_mapping_path))\n",
    "    return nodes_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88be4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node collections \n",
    "nodes_collection = generate_nodes_collection(data, main_config, taxa_mapping_path)\n",
    "\n",
    "# print(node_collections['Institution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848efb7c-d2a1-4022-95a4-8582b7ecb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doble check the number of nodes and _keys in each collection\n",
    "def check_nodes_keys(name_node_collection_list:list):\n",
    "    '''\n",
    "    Check the number of nodes and _keys in each collection.\n",
    "    '''\n",
    "    for collection in name_node_collection_list:\n",
    "        print(f\"Number of nodes in {collection}: {len(nodes_collection[collection])}\")\n",
    "        print(f\"Number of total _keys in {collection}: {sum(1 for x in nodes_collection[collection] if '_key' in x)}\") \n",
    "        print(f\"Number of unique _keys in {collection}: {len({x['_key'] for x in nodes_collection[collection] if '_key' in x})}\")\n",
    "\n",
    "\n",
    "nodes_list = ['Project', 'User', 'Country', 'Fermenter', 'Medium', 'Experiment', 'Run', 'Phase', 'Event', 'Institution']\n",
    "\n",
    "check_nodes_keys (nodes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e863d2a",
   "metadata": {},
   "source": [
    "### 3. Create edges collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f92034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge Function Definitions\n",
    "def get_institution_static_edges(nodes_collection:dict, edges_collection:dict) -> dict:\n",
    "    project = nodes_collection['Project']\n",
    "    user = nodes_collection['User']\n",
    "    country = nodes_collection['Country']\n",
    "    institution = nodes_collection['Institution']\n",
    "\n",
    "    edges_collection.update({\n",
    "        'created_at': {'edges':[{\n",
    "                        '_from': f\"Project/{project}['_key]\",\n",
    "                        '_to': f\"Institution/{institution}['_key]\"\n",
    "                        }],\n",
    "                'from_collection': ['Project'],\n",
    "                'to_collection': ['Institution']},\n",
    "        'from':{'edges':[{\n",
    "                        '_from': f\"Institution/{institution}['_key]\",\n",
    "                        '_to': f\"Country/{country}['_key]\"\n",
    "                        }],\n",
    "                'from_collection': ['Institution'],\n",
    "                'to_collection': ['Country']},\n",
    "        'works_at': {'edges':[{\n",
    "                            '_from': f\"User/{user}['_key]\",\n",
    "                            '_to': f\"Institution/{institution}['_key]\"\n",
    "                            }],\n",
    "                     'from_collection': ['User'],\n",
    "                     'to_collection': ['Institution']}\n",
    "        \n",
    "    })\n",
    "    return edges_collection\n",
    "\n",
    "def get_project_static_edges(nodes_collection:dict, edges_collection:dict) -> dict:\n",
    "    projects = nodes_collection['Project']\n",
    "    users = nodes_collection['User']\n",
    "\n",
    "    edges_collection.update({\n",
    "        'created_by': {'edges': [],\n",
    "                       'from_collection': ['Project'],\n",
    "                       'to_collection': ['User']}\n",
    "    })\n",
    "\n",
    "    for user in users:\n",
    "        for project in projects:\n",
    "            if '_key' in project:\n",
    "                        edges_collection['created_by']['edges'].append({'_from': f\"Project/{project['_key']}\",\n",
    "                                                            '_to': f\"User/{user['_key']}\"})\n",
    "    return edges_collection\n",
    "\n",
    "def get_has_experiment_edge(nodes_collection:dict, edges_collection:dict) -> dict:\n",
    "    projects = nodes_collection['Project']\n",
    "    experiments = nodes_collection['Experiment']\n",
    "\n",
    "    edges_collection.update({\n",
    "        'has_experiment':{'edges':[],\n",
    "                          'from_collection': ['Project'],\n",
    "                          'to_collection': ['Experiment']}\n",
    "    })\n",
    "     \n",
    "    for project in projects:\n",
    "        for experiment in experiments:\n",
    "            if '_key' in experiment:\n",
    "                edges_collection['has_experiment']['edges'].append({'_from': f\"Project/{project['_key']}\",\n",
    "                                                                    '_to': f\"Experiment/{experiment['_key']}\"})\n",
    "\n",
    "    return edges_collection\n",
    "\n",
    "\n",
    "def get_experiment_edges(nodes_collection:dict, edges_collection:dict) -> dict:\n",
    "\n",
    "    experiment_edges = {}\n",
    "    \n",
    "    edges_collection.update(experiment_edges)\n",
    "    return edges_collection\n",
    "\n",
    "def get_belongsto_edges(nodes_collection:dict, edges_collection:dict, taxa_mapping_path:str):\n",
    "    with open(taxa_mapping_path, 'r') as taxa_file:\n",
    "        taxa_mapping = pd.read_csv(taxa_file, sep=',')\n",
    "    \n",
    "    # from df to dictinary structure\n",
    "    taxa_mapping = {r[0]: {'name': r[1], 'synonym': r[2], 'taxid': r[3]} for i,r in taxa_mapping.iterrows()}\n",
    "    \n",
    "    edges_collection.update({\n",
    "                'belongs_to':{'edges':[],\n",
    "                        'from_collection': ['Strain'],\n",
    "                        'to_collection': ['Taxa']}\n",
    "    })\n",
    "\n",
    "    for strain in nodes_collection['Strain']:\n",
    "        strain_key = strain['_key']\n",
    "        parent_strain = strain('parent')\n",
    "        if parent_strain in taxa_mapping:\n",
    "            taxa_id = str(taxa_mapping[parent_strain]['taxid'])\n",
    "            edges_collection['belongs_to']['edges'].append({\n",
    "                '_from': f\"Strain/{strain_key}\",\n",
    "                '_to': f\"Taxa/{taxa_id}\"\n",
    "            })\n",
    "    return edges_collection\n",
    "\n",
    "def get_isparentof_edges(nodes_collection:dict, edges_collection:dict):\n",
    "    edges_collection.update({\n",
    "                'is_parent_of':{'edges':[],\n",
    "                        'from_collection': ['Strain'],\n",
    "                        'to_collection': ['Strain']},\n",
    "    })\n",
    "\n",
    "    for strain in nodes_collection['Strain']:\n",
    "        strain_key = strain['_key']\n",
    "        parent_strain = strain['parent']\n",
    "        parent_key = get_hash(parent_strain, prefix=\"ST\")\n",
    "        strain_id = strain['id']\n",
    "        if parent_strain == strain_id:\n",
    "            edges_collection['is_pare_of']['edges'].append({\n",
    "                '_from': f\"Strain/{strain_key}\",\n",
    "                '_to': f\"Strain/{parent_key}\"\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09058848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating all edges\n",
    "\n",
    "def generate_edges_collection(nodes_collection:dict) -> dict:\n",
    "    '''\n",
    "    This functon creates a dictionary with the edge collections backbone.\n",
    "    \n",
    "    parameters:\n",
    "        node_collections (dict): dictionary with the node objects in FermentDB\n",
    "    \n",
    "    return:\n",
    "        edge_collections (dict): dictionary with the expected edge objects in FermentDB\n",
    "    '''\n",
    "    \n",
    "\n",
    "    edges_collection = {\n",
    "        'has_run':{'edges':[],\n",
    "                   'from_collection': ['Experiment'],\n",
    "                   'to_collection': ['Run']}, \n",
    "        'has_product':{'edges':[],\n",
    "                       'from_collection': ['Run'],\n",
    "                       'to_collection': ['Compound']},\n",
    "        'has_substrate':{'edges':[],\n",
    "                         'from_collection': ['Run'],\n",
    "                         'to_collection': ['Compound']},\n",
    "        'uses_fermenter':{'edges':[],\n",
    "                            'from_collection': ['Run'],\n",
    "                            'to_collection': ['Fermenter'],\n",
    "                            'reactor_id': []},\n",
    "        'cultures_strain':{'edges':[],\n",
    "                           'from_collection': ['Run'],\n",
    "                           'to_collection': ['Strain'],\n",
    "                           'strain_batch':[]},\n",
    "        'has_medium':{'edges':[],\n",
    "                      'from_collection': ['Run'],\n",
    "                      'to_collection': ['Medium'],\n",
    "                      'reference_volume':[],\n",
    "                      'unit':[],\n",
    "                      'ph':[]},\n",
    "        'derived_from':{'edges':[],\n",
    "                        'from_collection': ['Medium'],\n",
    "                        'to_collection': ['Medium']},\n",
    "        'has_ingredient':{'edges':[],\n",
    "                          'from_collection': ['Medium'],\n",
    "                          'to_collection': ['Compound'],\n",
    "                          'concentration':[],\n",
    "                          'unit':[]},\n",
    "        'has_measured_cultivation_cond':{'edges':[],\n",
    "                                         'from_collection': ['Run'],\n",
    "                                         'to_collection': ['Cultivation_cond'],\n",
    "                                         'data':[],\n",
    "                                         'is_categorical':[],\n",
    "                                         'unit':[]},\n",
    "        'has_measured_process_var':{'edges':[],\n",
    "                                    'from_collection': ['Run'],\n",
    "                                    'to_collection': ['Process_var'],\n",
    "                                    'data':[],\n",
    "                                    'data_format':[],\n",
    "                                    'unit':[],\n",
    "                                    'timestamp':[]},\n",
    "        'has_measured_compound': {'edges': [],\n",
    "                                  'from_collection': ['Run'],\n",
    "                                  'to_collection': ['Compound'],\n",
    "                                  'data':[],\n",
    "                                  'data_format':[],\n",
    "                                  'unit':[],\n",
    "                                  'timestamp':[]},\n",
    "        'associated_with': {'edges':[],\n",
    "                            'from_collection': ['Compound'],\n",
    "                            'to_collection':['Calculated_var']},\n",
    "        'has_calculated_var': {'edges':[],\n",
    "                               'from_collection': ['Run'],\n",
    "                               'to_collection': ['Calculated_var'],\n",
    "                               'data':[],\n",
    "                                'data_format':[],\n",
    "                                'unit':[],\n",
    "                                'timestamp':[]},\n",
    "        'has_calculated_imodulon': {'edges':[],\n",
    "                                    'from_collection': ['Run'],\n",
    "                                    'to_collection': ['iModulon'],\n",
    "                                    'data':[],\n",
    "                                    'data_format':[],\n",
    "                                    'timestamp':[]},\n",
    "        'has_calculated_phase': {'edges':[],\n",
    "                                'from_collection': ['Run'],\n",
    "                                'to_collection': ['Phase'],\n",
    "                                'calculated_start':[],\n",
    "                                'calculated_end':[]},\n",
    "        'is_event_of': {'edges':[],\n",
    "                        'from_collection': ['Event'],\n",
    "                        'to_collection': ['Phase'],\n",
    "                        'is_start':[]}           \n",
    "    }\n",
    "    \n",
    "    edges_collection.update(get_institution_static_edges(nodes_collection))\n",
    "    edges_collection.update(get_project_static_edges(nodes_collection))\n",
    "    edges_collection.update(get_experiment_edges(nodes_collection, edges_collection))\n",
    "    edges_collection.update(get_has_experiment_edge(nodes_collection, edges_collection))\n",
    "    edges_collection.update(get_belongsto_edges(nodes_collection, edges_collection, taxa_mapping_path))\n",
    "    edges_collection.update(get_isparentof_edges(nodes_collection, edges_collection))\n",
    "\n",
    "    return edges_collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all edges\n",
    "edges_collection = generate_edges_collection(nodes_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5f3f4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions\n",
    "\n",
    "def get_run_conditions(collections, rconfig):\n",
    "    '''\n",
    "    Update node collections structure with nodes: initial condition, process conditions, fermenter, and strain.\n",
    "    Generate edges structure with collections: has_initial_condition, has_condition, cultures_strain, and uses_fermenter.  \n",
    "    \n",
    "    parameter:\n",
    "    - collections (dict): A dictionary containing collections of data.\n",
    "\n",
    "    returns:\n",
    "    - edges (dict): A dictionary containing edges data representing the relationships between runs, conditions, and strain.\n",
    "    '''\n",
    "    iconditions_collection = []\n",
    "    pconditions_collection = []\n",
    "    fermenter_collection = []\n",
    "    strain_collection = []\n",
    "    edges = {'has_initial_condition': {'edges':[],\n",
    "                                        'from_collection': ['Run'],\n",
    "                                        'to_collection': ['Initial_condition']},\n",
    "                'has_condition': {'edges': [],\n",
    "                                'from_collection': ['Run'],\n",
    "                                'to_collection': ['Process_condition']},\n",
    "                'has_measured_imodulon': {'edges':[],\n",
    "                              'from_collection': ['Run'],\n",
    "                              'to_collection':['iModulon']},\n",
    "                'cultures_strain': {'edges': [],\n",
    "                                'from_collection': ['Run'],\n",
    "                                'to_collection': ['Strain']},\n",
    "                'uses_fermenter': {'edges': [],\n",
    "                                'from_collection': ['Run'],\n",
    "                                'to_collection': ['Fermenter']},\n",
    "            }\n",
    "    \n",
    "    for run in collections['Run']:\n",
    "        run[\"_key\"] = run[\"name\"]+\"_\"+str(run['id'])\n",
    "        run[\"_key\"] = get_hash(run[\"_key\"], prefix=\"R\")\n",
    "        for variable in run['variables']:\n",
    "            variable[\"_key\"] = str(variable['name'])\n",
    "            variable[\"_key\"] = get_hash(variable[\"_key\"], prefix=\"C\")\n",
    "            data = variable.pop('data')\n",
    "            timestamps = variable.pop('timestamps')\n",
    "            unit = variable.pop('unit')\n",
    "            _ = variable.pop('categorical_data')\n",
    "            _ = variable.pop('raw_data')\n",
    "            _ = variable.pop('datetime_data')\n",
    "            _ = variable.pop('errors')\n",
    "            \n",
    "            if variable['name'] in rconfig['Run']:\n",
    "                if type(data) == list:\n",
    "                    data = data[0][0]\n",
    "                if variable['name'] == \"Strain Batch\":\n",
    "                    strain = '_'.join(data.split('-')[:1])\n",
    "                    strain_key = get_hash(strain, prefix=\"S\")\n",
    "                    strain_collection.append({'_key': strain_key,\n",
    "                                            'name': strain,\n",
    "                                            'rank': 'strain'})\n",
    "                    edges['cultures_strain']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                                                            '_to': f'Strain/{strain_key}',\n",
    "                                                            'strain_batch': data})\n",
    "                run.update({rconfig['Run'][variable['name']]: data}) # Add to edge cultures strain and delete?\n",
    "            elif variable['name'] in rconfig['Fermenter']:\n",
    "                variable[\"_key\"] = get_hash(data, prefix=\"F\")\n",
    "                fermenter_collection.append({'_key': variable[\"_key\"],\n",
    "                                             'name': data})\n",
    "                edges['uses_fermenter']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                           '_to': f\"Fermenter/{variable['_key']}\"})\n",
    "            elif variable['name'] in rconfig['Initial_condition']:\n",
    "                iconditions_collection.append(variable)\n",
    "                edges['has_initial_condition']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                           '_to': f\"Initial_condition/{variable['_key']}\",\n",
    "                           'data': data,\n",
    "                           'unit': unit})\n",
    "            elif '_RNAseq' in variable['name']:\n",
    "                if not all(v == 0 for v in data):\n",
    "                    variable['name'] = ' '.join(variable['name'].replace('_RNAseq', '').split('_'))\n",
    "                    variable[\"_key\"] = get_hash(variable[\"name\"], prefix=\"iM\")\n",
    "                    edges['has_measured_imodulon']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                                                           '_to': f\"iModulon/{variable['_key']}\",\n",
    "                                                           'data': data,\n",
    "                                                           'timestamps': timestamps})\n",
    "            else:\n",
    "                pconditions_collection.append(variable)\n",
    "                if not all(v == 0 for v in data):\n",
    "                    edges['has_condition']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                            '_to': f\"Process_condition/{variable['_key']}\",\n",
    "                            'data': data,\n",
    "                            'unit': unit,\n",
    "                            'timestamps': timestamps})\n",
    "        del run['variables']\n",
    "    \n",
    "    iconditions_collection = get_unique_json_from_list_of_dicts(d=iconditions_collection, \n",
    "                                                                    unique_key='_key')\n",
    "    pconditions_collection = get_unique_json_from_list_of_dicts(d=pconditions_collection, \n",
    "                                                                    unique_key='_key')\n",
    "    fermenter_collection = get_unique_json_from_list_of_dicts(d=fermenter_collection, \n",
    "                                                                    unique_key='_key')\n",
    "    strain_collection = get_unique_json_from_list_of_dicts(d=strain_collection, \n",
    "                                                                    unique_key='_key')\n",
    "    collections['Initial_condition'] = iconditions_collection\n",
    "    collections['Process_condition'] = pconditions_collection\n",
    "    collections['Fermenter'] = fermenter_collection\n",
    "    collections['Strain'] = strain_collection\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def get_run_phases(collections):\n",
    "    '''\n",
    "    Retrieves phases data from Run collection and create Phase_event node collection and has_phase edge collection.  \n",
    "\n",
    "    parameter:\n",
    "    - collections (dict): A dictionary containing collections of data.\n",
    "\n",
    "    returns:\n",
    "    - dict: A dictionary containing edges data representing the relationships between runs and phases.\n",
    "    '''\n",
    "    phases_collection = []\n",
    "    edges = {'has_phase': {'edges':[],\n",
    "                           'from_collection': ['Run'],\n",
    "                           'to_collection':['Phase_event']}}\n",
    "\n",
    "    for run in collections['Run']:\n",
    "        for phase in run['phases']:\n",
    "            phase[\"_key\"] = phase['name']\n",
    "            phase[\"_key\"] = get_hash(phase[\"_key\"], prefix=\"PH\")\n",
    "            attributes = {\"event_start\": phase.pop(\"start\"),\n",
    "                          \"event_end\": phase.pop(\"end\"),\n",
    "                          \"comment\": phase.pop(\"comment\"),\n",
    "                          \"created_by\": phase.pop(\"created_by\"),\n",
    "                          \"start\": phase.pop(\"relative_start\"),\n",
    "                          \"end\": phase.pop(\"relative_end\")}\n",
    "            attributes.update({'_from': f\"Run/{run['_key']}\",\n",
    "                           '_to': f\"Phase_event/{phase['_key']}\"})\n",
    "            phases_collection.append(phase)\n",
    "            edges['has_phase']['edges'].append(attributes)\n",
    "            phases_collection = get_unique_json_from_list_of_dicts(d=phases_collection, \n",
    "                                                                    unique_key='_key')\n",
    "        del run['phases']\n",
    "    \n",
    "    collections['Phase_event'] = phases_collection\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e59fda",
   "metadata": {},
   "source": [
    "### 4. Create iModulon node and edges collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac1d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define iModulon function \n",
    "def get_imodulon_collection(organism, dataset, table_path):\n",
    "    '''\n",
    "    Process iModulon data to generate a collection for a given organism and dataset. \n",
    "    - Reads data from a CSV file into a pandas DataFrame. \n",
    "    - Adds additional fields such as a key column from hashing name column, and creates linkout column by constructing URLs to generate individual links for each iModulon entry.\n",
    "    - Processes dataset by handling misisng values and dropping the 'k' column as it is no loger needed (values areincorporated into the linkout column) \n",
    "\n",
    "    parameters:\n",
    "        organism (str): The name of the organism (e.g., 'e_coli').\n",
    "        dataset (str): The name of the dataset (e.g., 'precise1k').\n",
    "        table_path (str): The file path to the CSV file containing the iModulon data (e.g., '/path/to/iM_table.csv').\n",
    "\n",
    "    return:\n",
    "        data_dict: A list of dictionaries representing processed iModulon data\n",
    "    '''\n",
    "    imodulon_link = f'https://imodulondb.org/iModulon.html?organism={organism}&dataset={dataset}&k='\n",
    "    data = pd.read_csv(table_path, sep=',', header=0)\n",
    "    data['_key'] = data['name'].apply(lambda n: get_hash(n, prefix=\"iM\"))\n",
    "    data['linkout'] = data['k'].apply(lambda k: imodulon_link+str(k))\n",
    "    data = data.fillna('NaN').drop('k', axis=1)\n",
    "    data_dict = data.to_dict(orient='records')\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2282c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate iModulon collection\n",
    "collections['iModulon'] = []\n",
    "for organism in imodulon_files:\n",
    "    for dataset in imodulon_files[organism]:\n",
    "        imodulon_path = imodulon_files[organism][dataset]\n",
    "        collections['iModulon'].extend(get_imodulon_collection(organism=organism, dataset=dataset, table_path=imodulon_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ac80f",
   "metadata": {},
   "source": [
    "### 4. Output nodes and edges collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6c083fa-6595-47fa-98ce-8c9e6303ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output nodes collections \n",
    "os.makedirs(os.path.dirname(nodes_path), exist_ok = True)\n",
    "nodes_str = json.dumps(node_collections)\n",
    "with open(nodes_path, 'w') as out:\n",
    "    out.write(nodes_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc77c8fc-1de8-4d62-aa43-6e2671b3d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output edges collections\n",
    "os.makedirs(os.path.dirname(edges_path), exist_ok = True)\n",
    "edges_str = json.dumps(edge_collections)\n",
    "with open(edges_path, 'w') as out:\n",
    "    out.write(edges_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fermentdb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
