{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea153251",
   "metadata": {},
   "source": [
    "# Parser for PASX Data File\n",
    "This notebook focuses on parsing <a href=\"../data/CrashMS_high resolution HD experiment.json\"> PASX data file </a> to extract and model/organize high-cell density fermentation data.  \n",
    "\n",
    "PASX file contains data about one project which contains several batches of different runs with data of several features.   \n",
    "\n",
    "The <b>primary objective</b> is to parse the PASX file with configs files such as parser_config and run_config in order to generate dictionaries for our graph, concretely node and edge collections such as: project, batch_cell_cultures, run, strain, species, initial_conditions and process_conditions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b96a6a9-cc90-46f7-9a29-ed9e73623d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install dependencies\n",
    "# !pip install pandas\n",
    "# !pip install pyarango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "33ae25ad-ad5c-4bff-957c-1d9a124d7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import modules\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "from functools import reduce\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "30c3e111-55d5-4282-a0eb-515d579d85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define file paths\n",
    "data_dir = '../data'\n",
    "data_path = os.path.join(data_dir, \"CrashMS_high resolution HD experiment.json\")\n",
    "config_dir = '../config'\n",
    "config_path = os.path.join(config_dir, 'parser_config.yaml')\n",
    "run_config_path = os.path.join(config_dir, 'run_config.yaml')\n",
    "output_dir = '../output'\n",
    "nodes_path = os.path.join(output_dir, \"fermentdb_nodes_full.json\")\n",
    "edges_path = os.path.join(output_dir, \"fermentdb_edges_full.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5b857",
   "metadata": {},
   "source": [
    "### 1. Load data and config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e34617eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['modification_time', 'result_notes', 'tags', 'departments', 'event_names', 'start', 'key_variable', 'editable', 'sites', 'creation_time', 'end', 'workflow', 'created_by', 'manager', 'trending_settings', 'editable_plots', 'country', 'unit_operations', 'name', 'description', 'deletable', 'type', 'batch_phase_names', 'project', 'progress', 'id', 'status', 'batches'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PASX data file\n",
    "with open(data_path, 'r') as dbfile:\n",
    "    data = json.load(dbfile)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "42640478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Project': {'_key': 'project',\n",
       "  'id': 'project',\n",
       "  'name': 'name',\n",
       "  'description': 'description',\n",
       "  'progress': 'progress',\n",
       "  'status': 'status',\n",
       "  'deletable': 'deletable',\n",
       "  'editable': 'editable',\n",
       "  'type': 'type',\n",
       "  'creation_time': 'creation_time',\n",
       "  'tags': 'tags',\n",
       "  'modification_time': 'modification_time',\n",
       "  'start': 'start',\n",
       "  'end': 'end'},\n",
       " 'Country': {'_key': 'country', 'id': 'country', 'name': 'country'},\n",
       " 'User': {'_key': 'manager/username',\n",
       "  'id': 'manager/username',\n",
       "  'name': 'manager/first_name',\n",
       "  'surname': 'manager/last_name'},\n",
       " 'Batch_cell_culture': {'_key': 'id', 'id': 'id', 'name': 'name'},\n",
       " 'Run': {'batches': {'id': 'id',\n",
       "   'name': 'name',\n",
       "   'description': 'description',\n",
       "   'creation_time': 'creation_time',\n",
       "   'modification_time': 'modification_time',\n",
       "   'batch_start': 'batch_start',\n",
       "   'batch_end': 'batch_end',\n",
       "   'first_timestamp': 'first_timestamp',\n",
       "   'last_timestamp': 'last_timestamp',\n",
       "   'variables': 'variables',\n",
       "   'phases': 'phases'}}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parser_config.yaml\n",
    "with open(config_path, 'r') as cfile:\n",
    "    config = yaml.load(cfile, Loader=yaml.SafeLoader)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6d0c465e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Run': {'id': 'id',\n",
       "  'Culture Type': 'culture_type',\n",
       "  'Seed': 'seed',\n",
       "  'Sample ID': 'sample_id',\n",
       "  'Experiment': 'experiment',\n",
       "  'Replicate #': 'replicate_number',\n",
       "  'Strain Batch': 'strain_batch',\n",
       "  'Container ID (calculated)': 'container_id',\n",
       "  'Reactor/Plate/Flask Number': 'reactor_plate_flask_number',\n",
       "  'Control?': 'is_control',\n",
       "  'Comments': 'comments',\n",
       "  'Inducer': 'inducer',\n",
       "  'Volume Unit_INDUCTION': 'volume_unit_induction',\n",
       "  'Condition': 'condition'},\n",
       " 'Fermenter': {'Container Type': 'name'},\n",
       " 'Initial_condition': {'SOP': 'SOP',\n",
       "  'Aeration Gas Type': 'aeration_gas_type',\n",
       "  'Aeration Profile': 'aeration_profile',\n",
       "  'Maximum Aeration (slpm)': 'maximum_aeration_slpm',\n",
       "  'Minimum Aeration (slpm)': 'minimum_aeration_slpm',\n",
       "  'Maximum Stirring or Shaking Speed (rpm)': 'maximum_stirring_speed_rpm',\n",
       "  'Minimum Stirring or Shaking Speed (rpm)': 'minimum_stirring_speed_rpm',\n",
       "  'DO Control Setpoint (%)': 'DO_control_setpoint_percentage',\n",
       "  'DO Control Cascade Level 1': 'DO_control_cascade_level1',\n",
       "  'DO Control Cascade Level 2': 'DO_control_cascade_level2',\n",
       "  'DO Control Cascade Level 3': 'DO_control_cascade_level3',\n",
       "  'Starting OD': 'starting_OD',\n",
       "  'Feed #1 Profile': 'feed_1_profile',\n",
       "  'Feed Medium #1': 'feed_medium_1',\n",
       "  'Feed Medium #2': 'feed_medium_2',\n",
       "  'pH Setpoint': 'pH_setpoint',\n",
       "  'pH Control Base Solution': 'ph_control_base_solution',\n",
       "  'pH Control Acid Solution': 'ph_control_acid_solution',\n",
       "  'Temperature Setpoint (C)': 'temperature_setpoint_celsius',\n",
       "  'Main Culture ID': 'main_culture_id',\n",
       "  'Initial Culture VolumeNone': 'initial_volume',\n",
       "  'Culture Volume Unit': 'culture_volume_unit',\n",
       "  'Base Medium': 'base_medium'}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load run_config.yaml\n",
    "with open(run_config_path, 'r') as cfile:\n",
    "    rconfig = yaml.load(cfile, Loader=yaml.SafeLoader)\n",
    "\n",
    "rconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29be854",
   "metadata": {},
   "source": [
    "### 2. Create node collections dictionary\n",
    "Create a dictionary for nodes collections with parser_config file and PASX data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "03a4becc-79a7-4191-a505-decba440eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions\n",
    "\n",
    "def get_from_nested_dict(data_dict:dict, map_list:list):\n",
    "    '''\n",
    "    Extracts value from a nested dictionary given a list of keys (different levels).\n",
    "\n",
    "    parameters:\n",
    "        data_dict (dict): dictionary structure \n",
    "        map_list (list): list of nested keys\n",
    "\n",
    "    return:\n",
    "        nested_value: nested value given the provided list of keys\n",
    "\n",
    "    example:\n",
    "        >>> data_dict = {'a': {'b': {'c': 5}}}\n",
    "        >>> value = get_from_nested_dict(data_dict=data_dict, map_list=['a', 'b', 'c'])\n",
    "        >>> print(value)\n",
    "        5\n",
    "    '''\n",
    "\n",
    "    nested_value = None\n",
    "    try:\n",
    "        nested_value = reduce(operator.getitem, map_list, data_dict)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return nested_value\n",
    "\n",
    "\n",
    "def get_collections_from_pasx(data: dict, config: dict) -> dict:\n",
    "    '''\n",
    "    This function creates a dictionary with the node collections expected\n",
    "    in FermentDB. It requires a dictionary with the data exported from PASX\n",
    "    in json format, and a configuration file that specifies the mapping between\n",
    "    PASX and FermentDB structure. See example in '/config/parser_config.yaml'\n",
    "\n",
    "    parameters:\n",
    "        data (dict): dictionary with the data read from PASX in json format\n",
    "        config (dict): mapping configuration to adapt to FermentDB structure\n",
    "\n",
    "    return:\n",
    "        collections (dict): dictionary with the expected node objects in FermentDB\n",
    "\n",
    "    example:\n",
    "        >>> get_collections_from_pasx(data=pasx_json_dict, config=parser_config.yaml)\n",
    "    '''\n",
    "    collections = {}\n",
    "    for c in config:\n",
    "        collections[c] = {}\n",
    "        for a in config[c]:\n",
    "            if type(config[c][a]) != dict:\n",
    "                key = config[c][a].split('/')\n",
    "                value = get_from_nested_dict(data, key)\n",
    "                if a == '_key':\n",
    "                    value = str(value)\n",
    "                collections[c].update({a: value})\n",
    "            else:\n",
    "                collections[c] = []\n",
    "                for nested_data in data[a]:\n",
    "                    nested_collection = {}\n",
    "                    for nested_a in config[c][a]:\n",
    "                        key = config[c][a][nested_a].split('/')\n",
    "                        value = get_from_nested_dict(nested_data, key)\n",
    "                        if nested_a == '_key':\n",
    "                            value = str(value)\n",
    "                        nested_collection.update({nested_a: value})\n",
    "                    collections[c].append(nested_collection)\n",
    "                    \n",
    "    return collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848efb7c-d2a1-4022-95a4-8582b7ecb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node collections \n",
    "collections = get_collections_from_pasx(data, config)\n",
    "\n",
    "print(collections[\"Project\"])\n",
    "print(collections[\"Batch_cell_culture\"])\n",
    "print(collections[\"Run\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e863d2a",
   "metadata": {},
   "source": [
    "### 3. Create edges collection dictionary \n",
    "- Extract variables and phases from Run Collection and create collections: initial_conditions, process_condition, Strain and phase_event. \n",
    "- Create a dictionary for edges collections with run_config file.\n",
    "- Create species collection and edge between Strain and Species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f3f4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions\n",
    "\n",
    "def generate_identifier():\n",
    "    '''\n",
    "    Generate a unique identifier using UUID version 1.\n",
    "\n",
    "    return: \n",
    "        str: a unique identifier.\n",
    "    \n",
    "    example:\n",
    "        >>> generate_identifier()\n",
    "        '2c06784e-888d-11ec-9183-080027772c11'\n",
    "    '''\n",
    "    identifier = str(uuid.uuid1())\n",
    "    \n",
    "    return identifier\n",
    "\n",
    "\n",
    "def get_unique_json_from_list_of_dicts(d, unique_key='id'):\n",
    "    '''\n",
    "    Get a list of unique dictionaries based on a specified key.\n",
    "    \n",
    "    parameters:\n",
    "        d (list): list of dictionaries.\n",
    "        unique_key (str, optional): Key to determine uniqueness. Default to 'id'.\n",
    "    \n",
    "    return: \n",
    "        list: list of unique dictionaries. \n",
    "    \n",
    "    example:\n",
    "        >>> example_list = [\n",
    "        ...     {\"id\": 1, \"name\": \"Alice\"},\n",
    "        ...     {\"id\": 2, \"name\": \"Bob\"},\n",
    "        ...     {\"id\": 1, \"name\": \"Charlie\"},  # Duplicate id\n",
    "        ...     {\"id\": 3, \"name\": \"Alice\"}     # Duplicate name\n",
    "        ... ]\n",
    "        >>> get_unique_json_from_list_of_dicts(example_list)\n",
    "        #Intermediate step:\n",
    "        # {1: {\"id\": 1, \"name\": \"Alice\"}, 2: {\"id\": 2, \"name\": \"Bob\"}, 3: {\"id\": 3, \"name\": Alice}}.\n",
    "        [{'id': 1, 'name': 'Alice'}, {'id': 2, 'name': 'Bob'}, {'id': 3, 'name': 'Alice'}]\n",
    "    '''\n",
    "    unique_list = list({v[unique_key]:v for v in d}.values())\n",
    "    \n",
    "    return unique_list\n",
    "\n",
    "\n",
    "def get_hash(key, prefix=\"\"):\n",
    "    '''\n",
    "    Get a hash value for a given key:\n",
    "    Hash a string using SHA-1, before encode key string into bytes using the UTF-8 encoding, as the sha1() expects bytes as input. \n",
    "    Convert the binary hash value into a hexadecimal string and then into an 8-digit integer.\n",
    "    Convert it to a string and add a prefix.\n",
    "\n",
    "    parameters:\n",
    "        key (str): Input string to be hashed.\n",
    "        prefix (str, optional): Prefix to prepend to the hash value. Defaults to \"\".\n",
    "\n",
    "    return:\n",
    "        str: Hash value.\n",
    "\n",
    "    example:\n",
    "        >>> input_key = \"example_key\"\n",
    "        >>> get_hash(input_key, prefix=\"HASH_\")\n",
    "        'HASH_45200d86'\n",
    "    '''\n",
    "    hkey = str(int(hashlib.sha1(key.encode(\"utf-8\")).hexdigest(),16) % (10 ** 8))\n",
    "    hkey = f\"{prefix}{hkey}\"\n",
    "    \n",
    "    return hkey\n",
    "\n",
    "\n",
    "def get_run_conditions(collections):\n",
    "    '''\n",
    "    Generate collection nodes and collection edged for: initial condition, process conditions and strain.\n",
    "    Create a hash value for a given '_key' adding prefix (R: Run, C: condition, S: Strain Batch\n",
    "    \n",
    "    parameter:\n",
    "    - collections (dict): A dictionary containing collections of data.\n",
    "\n",
    "    returns:\n",
    "    - dict: A dictionary containing edges data representing the relationships between runs, conditions, and strain.\n",
    "    '''\n",
    "    iconditions_collection = []\n",
    "    pconditions_collection = []\n",
    "    fermenter_collection = []\n",
    "    strain_mapping = {}\n",
    "    fake_name_counter = 1\n",
    "    edges = {'has_initial_condition': {'edges':[],\n",
    "                                        'from_collection': ['Run'],\n",
    "                                        'to_collection': ['Initial_condition']},\n",
    "                'has_condition': {'edges': [],\n",
    "                                'from_collection': ['Run'],\n",
    "                                'to_collection': ['Process_condition']},\n",
    "                'cultures_strain': {'edges': [],\n",
    "                                'from_collection': ['Run'],\n",
    "                                'to_collection': ['Strain']},\n",
    "                'uses_fermenter': {'edges': [],\n",
    "                                'from_collection': ['Run'],\n",
    "                                'to_collection': ['Fermenter']},\n",
    "            }\n",
    "    \n",
    "    for run in collections['Run']:\n",
    "        run[\"_key\"] = run[\"name\"]+\"_\"+str(run['id'])\n",
    "        run[\"_key\"] = get_hash(run[\"_key\"], prefix=\"R\")\n",
    "        for variable in run['variables']:\n",
    "            variable[\"_key\"] = str(variable['name'])\n",
    "            variable[\"_key\"] = get_hash(variable[\"_key\"], prefix=\"C\")\n",
    "            data = variable.pop('data')\n",
    "            timestamps = variable.pop('timestamps')\n",
    "            unit = variable.pop('unit')\n",
    "            _ = variable.pop('categorical_data')\n",
    "            _ = variable.pop('raw_data')\n",
    "            _ = variable.pop('datetime_data')\n",
    "            _ = variable.pop('errors')\n",
    "            \n",
    "            if variable['name'] in rconfig['Run']:\n",
    "                if type(data) == list:\n",
    "                    data = data[0][0]\n",
    "                if variable['name'] == \"Strain Batch\":\n",
    "                    # Create fake strain\n",
    "                    original_strain = '_'.join(data.split('-')[:1])\n",
    "                    if original_strain not in strain_mapping:\n",
    "                        fake_name = f\"Strain{fake_name_counter}\"\n",
    "                        strain_mapping[original_strain] = fake_name\n",
    "                        fake_name_counter += 1\n",
    "                    else:\n",
    "                        fake_name = strain_mapping[original_strain]\n",
    "                    strain = fake_name\n",
    "                    strain_key = get_hash(strain, prefix=\"S\")\n",
    "                    if 'Strain' not in collections: # Create Strain collection\n",
    "                        collections['Strain'] = []\n",
    "                    collections['Strain'].append({'_key': strain_key,\n",
    "                                                'name': strain,\n",
    "                                                'rank': 'strain'})\n",
    "                    edges['cultures_strain']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                                                            '_to': f'Strain/{strain_key}',\n",
    "                                                            'strain_batch': data})\n",
    "                run.update({rconfig['Run'][variable['name']]: data})\n",
    "            elif variable['name'] in rconfig['Fermenter']:\n",
    "                variable[\"_key\"] = get_hash(data, prefix=\"F\")\n",
    "                fermenter_collection.append({'_key': variable[\"_key\"],\n",
    "                                             'name': data})\n",
    "                edges['uses_fermenter']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                           '_to': f\"Fermenter/{variable['_key']}\"})\n",
    "            elif variable['name'] in rconfig['Initial_condition']:\n",
    "                iconditions_collection.append(variable)\n",
    "                edges['has_initial_condition']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                           '_to': f\"Initial_condition/{variable['_key']}\",\n",
    "                           'data': data,\n",
    "                           'unit': unit})\n",
    "            else:\n",
    "                pconditions_collection.append(variable)\n",
    "                if not all(v == 0 for v in data):\n",
    "                    edges['has_condition']['edges'].append({'_from': f\"Run/{run['_key']}\",\n",
    "                            '_to': f\"Process_condition/{variable['_key']}\",\n",
    "                            'data': data,\n",
    "                            'unit': unit,\n",
    "                            'timestamps': timestamps})\n",
    "        del run['variables']\n",
    "    \n",
    "    iconditions_collection = get_unique_json_from_list_of_dicts(d=iconditions_collection, \n",
    "                                                                    unique_key='_key')\n",
    "    pconditions_collection = get_unique_json_from_list_of_dicts(d=pconditions_collection, \n",
    "                                                                    unique_key='_key')\n",
    "    fermenter_collection = get_unique_json_from_list_of_dicts(d=fermenter_collection, \n",
    "                                                                    unique_key='_key')\n",
    "    collections['Strain'] = get_unique_json_from_list_of_dicts(d=collections['Strain'], \n",
    "                                                                    unique_key='_key')\n",
    "    collections['Initial_condition'] = iconditions_collection\n",
    "    collections['Process_condition'] = pconditions_collection\n",
    "    collections['Fermenter'] = fermenter_collection\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def get_run_phases(collections):\n",
    "    '''\n",
    "    Retrieves phases data from the given collections and prepares it for database insertion.\n",
    "\n",
    "    parameter:\n",
    "    - collections (dict): A dictionary containing collections of data.\n",
    "\n",
    "    returns:\n",
    "    - dict: A dictionary containing edges data representing the relationships between runs and phases.\n",
    "    '''\n",
    "    phases_collection = []\n",
    "    edges = {'has_phase': {'edges':[],\n",
    "                           'from_collection': ['Run'],\n",
    "                           'to_collection':['Phase_event']}}\n",
    "\n",
    "    for run in collections['Run']:\n",
    "        for phase in run['phases']:\n",
    "            phase[\"_key\"] = phase['name']\n",
    "            phase[\"_key\"] = get_hash(phase[\"_key\"], prefix=\"P\")\n",
    "            attributes = {\"event_start\": phase.pop(\"start\"),\n",
    "                          \"event_end\": phase.pop(\"end\"),\n",
    "                          \"comment\": phase.pop(\"comment\"),\n",
    "                          \"created_by\": phase.pop(\"created_by\"),\n",
    "                          \"start\": phase.pop(\"relative_start\"),\n",
    "                          \"end\": phase.pop(\"relative_end\")}\n",
    "            attributes.update({'_from': f\"Run/{run['_key']}\",\n",
    "                           '_to': f\"Phase_event/{phase['_key']}\"})\n",
    "            phases_collection.append(phase)\n",
    "            edges['has_phase']['edges'].append(attributes)\n",
    "            phases_collection = get_unique_json_from_list_of_dicts(d=phases_collection, \n",
    "                                                                    unique_key='_key')\n",
    "        del run['phases']\n",
    "    \n",
    "    collections['Phase_event'] = phases_collection\n",
    "\n",
    "    return edges\n",
    "    \n",
    "\n",
    "def generate_project_edges(collections, edges):\n",
    "    '''\n",
    "    Generate edges between user, project and batch_cell_culture nodes. collections in a project graph.\n",
    "    \n",
    "    parameter: \n",
    "        collections (dict): A dictionary containing collections as keys and their corresponding documents as values.\n",
    "        edges(dict): A dictionary representing edges between collections.\n",
    "    \n",
    "    returns:\n",
    "        None: This function updates the 'edges' dictionary in place.\n",
    "    \n",
    "    example:\n",
    "        collections = {\n",
    "        'Project': {'_key': 'project_key', 'creation_time': '2024-05-01T12:00:00Z'},\n",
    "        'User': {'_key': 'user_key'},\n",
    "        'Batch_cell_culture': {'_key': 'batch_key'},\n",
    "        'Run': [{'_key': 'run_key1'}, {'_key': 'run_key2'}, ...]\n",
    "        }\n",
    "        edges = {}\n",
    "        generate_project_edges(collections, edges)\n",
    "        # edges will be updated with edges between collections.\n",
    "    '''\n",
    "    project = collections['Project']\n",
    "    user = collections['User']\n",
    "    batch = collections['Batch_cell_culture']\n",
    "    \n",
    "    edges.update({'has_batch': {'edges':[{'_from': f\"Project/{project['_key']}\",\n",
    "                                          '_to': f\"Batch_cell_culture/{batch['_key']}\"}], \n",
    "                                'from_collection': ['Project'],\n",
    "                                'to_collection':['Batch_cell_culture']},\n",
    "                  'created_by': {'edges':[{'_from': f\"Project/{project['_key']}\",\n",
    "                                          '_to': f\"User/{user['_key']}\",\n",
    "                                          'creation_date': project['creation_time']}],\n",
    "                                'from_collection': ['Project'],\n",
    "                                'to_collection':['User']}})\n",
    "    edges.update({'has_run': {'edges':[], \n",
    "                              'from_collection': ['Batch_cell_culture'],\n",
    "                                'to_collection':['Run']}})\n",
    "    for run in collections['Run']:\n",
    "        edges['has_run']['edges'].append({'_from': f\"Batch_cell_culture/{batch['_key']}\",\n",
    "                                          '_to': f\"Run/{run['_key']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e3667915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary edges with conditions collections, phases collections and project edges\n",
    "\n",
    "edges = get_run_conditions(collections)\n",
    "edges.update(get_run_phases(collections))\n",
    "generate_project_edges(collections, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0055c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Species document and create an edge 'belongs_to' between a Strain document and Species document. \n",
    "\n",
    "species_key = get_hash(\"562\", prefix=\"SP\")\n",
    "collections['Species'] = [{'_key': species_key,\n",
    "                          'name': 'Escherichia coli',\n",
    "                          'rank': 'species'}]\n",
    "if 'belongs_to' not in edges:\n",
    "    edges['belongs_to'] = {\n",
    "        'edges': [],\n",
    "        'from_collection': ['Strain'],\n",
    "        'to_collection': ['Species']\n",
    "    }\n",
    "\n",
    "for strain in collections['Strain']:\n",
    "    strain_key = strain['_key']\n",
    "    edges['belongs_to']['edges'].append({\n",
    "        '_from': f\"Strain/{strain_key}\",\n",
    "        '_to': f'Species/{species_key}'\n",
    "    }) \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "39c25ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an Institution node document and create their edges: works at, from and created_at.\n",
    "institution_key = get_hash(\"NNFCB\", prefix=\"I\")\n",
    "collections['Institution'] = {'_key': {institution_key},\n",
    "                              'name': 'NNFCB - Novo Nordisk Foundation Center for Biosustainability (DTU Biosustain)',\n",
    "                              'address': 'Building 220, Kemitorvet. 2800 Kgs. Lyngby',\n",
    "                              'email': 'biosustain@biosustain.dtu.dk',\n",
    "                              'phone_number':'+45 45 25 80 00'\n",
    "                              }\n",
    "\n",
    "## Create 'works_at' edge\n",
    "user_key = get_hash(\"sursud@biosustain.dtu.dk\", prefix=\"U\")\n",
    "edges['works_at'] = {'edges':[{'_from': f\"User/{user_key}\",\n",
    "                                '_to': f'Institution/{institution_key}'}],\n",
    "                    'from_collection': ['User'],\n",
    "                    'to_collection': ['Institution']}\n",
    "\n",
    "## Create 'created_at' edge\n",
    "project_key = get_hash(\"15\", prefix=\"P\")\n",
    "edges['created_at'] = {'edges':[{'_from': f'Project/{project_key}',\n",
    "                                '_to': f'Institution/{institution_key}'}],\n",
    "                        'from_collection': ['Project'],\n",
    "                        'to_collection': ['Institution']}\n",
    "\n",
    "## Create 'from' edge\n",
    "country_key = get_hash(\"DK\", prefix=\"C\")\n",
    "edges['from'] = {'edges': [{'_from': f'Institution/{institution_key}',\n",
    "                            '_to': f'Country/{country_key}'}],\n",
    "                'from_collection': ['Institution'],\n",
    "                'to_collection': ['Country']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ac80f",
   "metadata": {},
   "source": [
    "3. Output nodes and edges dictionaries as JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c6c083fa-6595-47fa-98ce-8c9e6303ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output node dictionary \n",
    "serializable_collections = {key: list(value) for key, value in collections.items()}\n",
    "\n",
    "nodes_str = json.dumps(serializable_collections)\n",
    "with open(nodes_path, 'w') as out:\n",
    "    out.write(nodes_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cc77c8fc-1de8-4d62-aa43-6e2671b3d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output edge dictionary\n",
    "\n",
    "edges_str = json.dumps(edges)\n",
    "with open(edges_path, 'w') as out:\n",
    "    out.write(edges_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
